
<html lang="en">
	<head>

		<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
		<meta http-equiv="content-script-type" content="text/javascript" />
		<meta http-equiv="content-style-type" content="text/css" />
		<meta http-equiv="content-language" content="nl" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0" />

		<meta name="author" content="Farita Tasnim" />
		<meta name="description" content="I'm Farita Tasnim, a researcher in the theoretical physics of living systems." />
		<meta name="keywords" content="physics, living systems, physics of living systems, hardware, solutions, software, integration, products, energy, farita, tasnim" />
		<link rel="icon" href="favicon.ico"/>
		<link rel="shortcut icon" href="http://farita.me/favicon.ico" />

		<title>Probability based on energy levels</title>

		<link href='http://fonts.googleapis.com/css?family=Droid+Serif' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Droid+Sans' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,300'rel='stylesheet' type='text/css'>
		<link href="/css/style.css" rel="stylesheet" />
        <script src="https://kit.fontawesome.com/7c2f268794.js" crossorigin="anonymous"></script>
        <script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(~', '\\)']]
              },
              svg: {
                fontCache: 'global'
              }
            };
        </script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
        
	</head>
    
    
	<body>
        <div class="frontpage">
            
        <div class="container" style="font-size:16pt;line-height:16px;font-family:'Open Sans'"> 
            
        <br> <br>
            
        <br> <br>
            
        Probability based on energy levels

        </div>
            
            
        <div class="container" style="font-size:11pt;line-height:16px;font-family:'Open Sans'">
            
            <br> <br>
            
            <p> We know Grogu's different energy levels. We also know that Grogu, like all systems in steady state, minimizes his energy.  <br> <br> </p> 
            
            <p> If Grogu tries to minimize his energy shouldn't he always be sleeping? That is, isn't it the case that if we open the box at a random time, the probability that he is sleeping is 1 and the probability that he is in any of the other higher energy states is zero? If there is nothing driving Grogu to change state, then actually, yes! However, we know that there must be some factor that is driving him to change state. Let's call this driving variable his irritability, $I$, which has some real value greater than or equal to zero. The greater Grogu's irritability, the more frequently he changes his state. <br> <br> </p> 
            
            <img src = "irritability.png" style="width:800px;">
            
            <p> --- Q --- If Grogu's irritability $I \to 0$, what is his steady state probability distribution? <br> <br>
                
                <input type="checkbox" value="A"> delta distribution centered at $E_1$ *<br>

                <input type="checkbox" value="B"> delta distribution centered at $E_5$<br>

                <input type="checkbox" value="C"> uniform distribution <br>
    
            <p> Right! Grogu would always remain in his lowest energy state if nothing drives him to change state. <br> <br> </p> 
            
            <p> --- Q --- If Grogu's irritability $I \to \infty$, what is his steady state probability distribution? <br> <br>
                
                <input type="checkbox" value="A"> delta distribution centered at $E_1$ <br>

                <input type="checkbox" value="B"> delta distribution centered at $E_5$<br>

                <input type="checkbox" value="C"> uniform distribution * <br>
    
            <p> Right! Grogu would constantly be changing state! So, he'd spend roughly equal amounts of time in each state. <br> <br> </p> 
            
            <p> How can we use this minimal set of knowledge, in addition to the mental trick of ensembles, to derive the probability that if we open the box at a random time, Grogu will be in a given one of his 5 possible energy states? Well, if we use ergodicity, remembering that  $$ P (\text{sleep}) = \lim_{N \to \infty} \frac{N(\text{sleep})}{N}  $$ then we would have to know how many of the Grogus-in-boxes in our ensemble consist of Grogu in each given energy state. So let's say we have a set $n = \{n_1, n_2, n_3, n_4, n_5 \}$. $n_1$ denotes the number of systems in the ensemble in which Grogu is in energy level $E_1$, $n_2$ the number of systems in which Grogu is in energy level $E_2$, and so on. <br> <br> </p> 
            
            <p> Let's say the total combined energy value of the ensemble of Grogus-in-boxes is E. This is a constraint on our ensemble. <br> <br> </p> 
            
            <p> --- Q --- What is the value of $\sum_j n_j$? <br> <br>
                
                <input type="checkbox" value="A"> $2N$ <br>

                <input type="checkbox" value="B"> $N$ * <br>

                <input type="checkbox" value="C"> $N / 2$ <br> <br>
                
            <p> --- Q --- What is the value of $\sum_j n_j E_j$? <br> <br>
                
                <input type="checkbox" value="A"> $2E$ <br>

                <input type="checkbox" value="B"> $E$ * <br>

                <input type="checkbox" value="C"> $E / 2$ <br> <br>
                
            <p> --- Q --- What's the probability that a randomly chosen Grogu in a box is in energy state $E_j$ given that we know the values in $n$? <br> <br>
                
                <input type="checkbox" value="A"> $P(E_j | n) = Nn_j $ <br> <br>

                <input type="checkbox" value="B"> $P(E_j | n) = \frac{n_j^2}{N^2} $ <br> <br>

                <input type="checkbox" value="C"> $P(E_j | n) = \frac{n_j}{N} $ * <br><br>
                
            <p> --- Q --- If our ensemble actually only consisted of 20 systems, and $n = \{7,5,1,3,4\}$, how many different such ensembles could exist? That is, how many ways could you arrange the 7 Grogus in boxes sleeping, the 5 stretching, the 1 walking, the 3 clawing, and the 4 clawing, among the 20 possible Grogus in boxes? <br> <br>
                
                <input type="checkbox" value="A"> $20!$ <br> <br>

                <input type="checkbox" value="B"> $\frac{20!}{7!5!1!3!4!}$ * <br> <br>
    
            <p> Great, but we don't know what $n$ is exactly, so we want to know $P(E_j)$, not conditioned on knowing the distribution of systems $n$.  <br> <br> </p> 
            
            <p> --- Q --- So now with unknown values, how many ensembles of size $N$ are consistent with $n$? Call this $\Omega(n)$. <br> <br>
                
                <input type="checkbox" value="A"> $N!$ <br> <br>

                <input type="checkbox" value="B"> $\Omega(n) = \frac{N!}{\prod_j n_j}$ * <br> <br>
            
            <p> Great. And the marginal probability can be written as $P(E_j) = \sum_n P(E_j | n) p(n)$ <br> <br> </p> 
            
            <p> --- Q --- So to solve for this, we need to know -- what is the probability the ensemble has some distribution $n$? <br> <br>
                
                <input type="checkbox" value="A"> $p(n) = \frac{\Omega(n)}{\sum_n \Omega(n)}$ * <br> <br>

                <input type="checkbox" value="B"> $p(n) = \frac{\Omega(n)}{\prod_n \Omega(n)}$ <br> <br>
            
            <p> Well it must be the case that it's the number of ways the ensemble has the distribution n versus the total number of ways the the ensemble can exist under any distribution $n$. <br> <br> </p> 
            
            <p> --- Q --- Putting all this together, what is an expression for the probability that Grogu is in state $E_j$? <br> <br>
                
                <input type="checkbox" value="A"> $P(E_j) = \frac{ \sum_n \Omega(n) n_j} { \sum_n \Omega(n) N }$ * <br> <br>

                <input type="checkbox" value="B"> $P(E_j) = \frac{ \prod_n \Omega(n) n_j} { \prod_n \Omega(n) N }$ <br> <br>
            
            <p> Another way to see this is that the probability that Grogu is in state $E_j$ is equal to the average fraction of the systems in our ensemble of Grogus-in-boxes that are in state $E_j$: $$P(E_j) = \frac{\langle n_j \rangle_{p(n)}}{N} $$ <br> <br> </p> 
            
            <p> Alright, so how do we make progress on calculating this probability, especially given the complicated formula for $\Omega(n)$? Can we make use of the fact that we're considering an infinitely large ensemble, that is $N \to \infty$? What can we say about the distribution $p(n)$ in this limit? Can the sums in the expression for $p(n)$ be replaced by something that's easily calculable? It turns out the answer is yes! If you haven't already learned about the <a class="title" href="maxterm.html" target = "_blank"><b>maximum term method</b></a>, please do so to understand how! <br> <br> </p> 
            
            <p> --- Q --- By the maximum term method, what does $\Omega(n)$ look like? <br> <br>
                
                <input type="checkbox" value="A"> a delta distribution centered at the $n^*$ with the lowest value of $\Omega(n)$ <br>

                <input type="checkbox" value="B"> a delta distribution centered at the $n^*$ with the highest value of $\Omega(n)$ * <br>
                
                <input type="checkbox" value="C"> a uniform distribution over all possible $n$ <br>
            
            <p> That's right - by the maximum term method, the $\sum_n \Omega(n)$ can simply be replaced by its maximum term, that is $\Omega(n^*)$.  <br> <br> </p> 
            
            <p> --- Q --- What would the expression for the probability that Grogu is in state $j$ become if we use the maximum term? <br> <br>
                
                <input type="checkbox" value="A"> $P(E_j) = \frac{n^*_j}{\Omega(n^*) N}$ <br> <br>

                <input type="checkbox" value="B"> $P(E_j) = \frac{\Omega(n^*) n^*_j}{\Omega(n^*) N} = \frac{n^*_j}{N}$ * <br> <br>
                
                <input type="checkbox" value="C"> $P(E_j) = \frac{n^*_j}{\Omega(n^*) }$ <br> <br>
            
            <p> --- Q --- So what would make sense to do next to find the actual value of the probability that Grogu is in state $E_j$? <br> <br>
                
                <input type="checkbox" value="A"> Calculate the distribution $\Omega(n)$ and use that to identify $n^*$ <br>

                <input type="checkbox" value="B"> Try to calculate $n^*$ directly without calculating the entire distribution $\Omega(n)$ * <br> <br>
            
            <p> Yes -  we can intuit that we may be able to do something like calculate the $n^*$ that maximizes $\Omega(n)$ based on experience from calculus, where we can often find the value of $x$ that maximizes or minimizes a function $f(x)$ by solving for when $f'(x) = 0$. We can do this without calculating the value of $f(x)$ for every value of $x$.  <br> <br> </p> 
            
            <p> It's just that we need to add in another layer to this kind of optimization because we have constraints on the distribution $n$. We can't just have any $n$. The $n$ must be such that $\sum_j n_j = N$, because there are a total of $N$ Grogus-in-boxes in our ensemble. Additionally, we have a constraint on the total energy of the system, so that $\sum_j n_j E_j = E$. So, we have to do *constrained* optimization using what's called the Lagrangian method, or the method of undetermined multipliers. If you haven't already learned about constrained optimization using the method of undetermined multipliers, please do so now. [TODO: Develop that page.] <br> <br> </p> 
            
            <p> Another trick that's very useful in statistical physics is what's called Stirling's approximation for the logarithm of a very large number. It relies on approximating a discrete sum as an integral when N is a very large integer: $$\ln N! = \sum_{i=0}^N \ln i \approx \int_{i=0}^N di \ln i = N \ln N - N$$ <br> <br> </p> 
            
            <p> --- Q --- What function would it make the *most* sense to maximize in our constrained optimization problem? Remember the goal is to work with functions that are as simple as possible.  <br> <br>
                
                <input type="checkbox" value="A"> $\Omega(n) - \alpha \sum_i n_i - \beta \sum_i n_i E_i$ <br> <br>

                <input type="checkbox" value="B"> $\ln \Omega(n) - \alpha \sum_i n_i - \beta \sum_i n_i E_i$ * <br> <br> 
                
                <input type="checkbox" value="C"> $\Omega(n) - \alpha \sum_i n_i$ <br> <br>

                <input type="checkbox" value="D"> $\ln \Omega(n) - \alpha \sum_i n_i$ <br> <br> 
                
                <input type="checkbox" value="E"> $\Omega(n) - \alpha \sum_i n_i E_i$ <br> <br>

                <input type="checkbox" value="F"> $\ln \Omega(n) - \alpha \sum_i n_i E_i$ <br> <br> 
            
            <p> Yes we have two constraints - on the total number of systems in the ensemble, and the total energy of the entire ensemble. While we could have chosen to maximize \Omega(n), it will be easier to take derivatives of $\ln \Omega(n)$. Firstly, this works because of the maximum term method. Secondly, maximizing $\ln \Omega(n)$ amounts to maximizing the <a class="title" href="entropy.html" target = "_blank"><b>entropy</b></a> of the system. Thirdly, we can apply Stirling's approximation for simplification, since we are considering the limit of large $N$.  <br> <br> </p> 
            
            <p> --- Q --- Using approximations, how can we rewrite $\lim_{N \to \infty} \ln \Omega(n)$? <br> <br>
                
                <input type="checkbox" value="A"> $(\sum_i n_i) \ln (\sum_i n_i) - \sum_i n_i \ln n_i$ * <br> <br>

                <input type="checkbox" value="B"> $(\sum_i n_i) \ln (\sum_i n_i)$ <br> <br> 
                
                <input type="checkbox" value="C"> $\sum_i n_i \ln n_i$ <br> <br>

                <input type="checkbox" value="D"> $-\sum_i n_i \ln n_i$ <br> <br> 
            
            <p> --- Q --- With respect to which variables will we take the partial derivatives of this function in the process of constrained optimization? (We will have to set these partial derivatives to $0$ and solve, just as we do when optimizing in calculus.) <br> <br>
                
                <input type="checkbox" value="A"> $E_j$ for all $j \in \{1,2,3,4,5\}$ <br>

                <input type="checkbox" value="B"> $n_j$ for all $j \in \{1,2,3,4,5\}$ * <br>
                
            <p> That's right, with respect to the number of Grogus-in-boxes in state $j$, for each possible state $j$ that Grogu could take on. So let's perform constrained optimization on the statistical variables describing our ensemble of Grogus-in-boxes. For each $j \in \{1,2,3,4,5\}$: $$\frac{\partial}{\partial n_j} (\ln \Omega(n) - \alpha \sum_i n_i - \beta \sum_i n_i E_i) = 0 $$ Substituting Stirling's approximation for $\ln \Omega(n)$, we obtain $$\ln(\sum_i n^*_i) - \ln n^*_j - \alpha - \beta E_j = 0$$ $$\ln N - \ln n^*_j - \alpha - \beta E_j = 0$$ Solving, we obtain that $$n^*_j = N e^{-\alpha} e^{-\beta E_j}$$ <br> <br> </p> 
            
            <p> --- Q --- We are very close now to the probability distribution $P(E_j)$. What variables must we still pin down in order to solve for it? <br> <br>
                
                <input type="checkbox" value="A"> $\alpha$ * <br>

                <input type="checkbox" value="B"> $\beta$ * <br>
                
                <input type="checkbox" value="C"> $E_j$ <br>
                
                <input type="checkbox" value="D"> $N$ <br>
            
            <p> How can we solve for $\alpha$ and $\beta$? Perhaps we can use the constraints for which they are coefficients? Let's plug our $n^*_j$ into these constraints. They are constraints after all - they should help constrain / define the variables in our equations. The first constraint on the number of systems: $\sum_j n^*_j  = N$. Plugging in for $n^*_j$, $$\sum_j N e^{-\alpha} e^{-\beta E_j} = N$$ From this we obtain that $$e^{-\alpha} = \frac{1}{\sum_j e^{-\beta E_j}} $$ This helps us refine our equation to $$n^*_j = \frac{N e^{-\beta E_j}}{\sum_j e^{-\beta E_j}}$$ So the probability that Grogu is in state $E_j$ can be written as  $$P(E_j) = \frac{n^*_j}{N} = \frac{e^{-\beta E_j}}{\sum_j e^{-\beta E_j}}$$ <br> <br> </p> 
            
            <p> We have just derived one of the most fundamental results of equilibrium statistical physics. When a system is in an equilibrium steady state, it occupies each of its possible states according to a distribution exponential with respect to the energy of each of those states. This energy-based distribution is famously referred to as the Boltzmann distribution. Here, play around with how changing Grogu's energy levels will change his probability distribution for a fixed $\beta$.<br> <br> </p> 
            
            <iframe src="https://www.wolframcloud.com/obj/a87a72f5-0609-4321-b01f-f7b48334bc43?_embed=iframe" width="800" height="600"></iframe>

            
            <p> If the above embedding doesn't work, try going to the following <a class="title" href="https://www.wolframcloud.com/obj/a87a72f5-0609-4321-b01f-f7b48334bc43" target = "_blank"><b>link</b></a>. You may have to log in to Wolfram. <br> <br> </p> 
            
            <p> To try to solve for \beta, we can use our final constraint, that $\sum_j n^*_j E_j = E$. So, $$\sum_j \frac{E_j e^{-\beta E_j}}{\sum_j e^{-\beta E_j}} = \frac{E}{N} = \langle E \rangle $$ which shows us that $\beta$ is an implicit function of Grogu's *average* energy. <br> <br> </p> 
            
            <p> In order to figure out exactly what \beta is, we need to understand <a class="title" href="entropy.html" target = "_blank"><b>entropy</b></a> and its <a class="title" href="conjugates.html" target = "_blank"><b>conjugate variables</b></a>. Additionally, when a system is in a steady state that is at equilibrium with its surroundings, not only does it minimize its energy, but it also maximizes its entropy. What that essentially means is that we cannot assume we know any more than the minimum about a system. That is, we cannot assume that our uncertainty about a system is any less than the maximum it could possibly be. If we do, then we rely on hope, for which we currently have no theories :) When we understand the implications of entropy maximization, we will find out what $\beta$ is. That's the next step to round out our understanding of this fundamental distribution. <br> <br> </p> 
            
            <p> <br> <br> </p> 
            
            <p> <br> <br> </p> 
            
            
        </div>
            
    </div>


    <script type="text/javascript">
        function toggle_visibility(id) {
            var e = document.getElementById(id);
            if(e.style.display == 'block')
                e.style.display = 'none';
            else
                e.style.display = 'block';
        }
    </script>
    
        
        
    </body>

</html>
    