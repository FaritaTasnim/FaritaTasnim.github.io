<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Farita | PoLS</title>
<link href='http://fonts.googleapis.com/css?family=Droid+Serif' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Droid+Sans' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300'rel='stylesheet' type='text/css'>
		<link href="/css/style.css" rel="stylesheet" />
        <script src="https://kit.fontawesome.com/3d48ed8956.js" crossorigin="anonymous"></script>
        <script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(~', '\\)']]
              },
              svg: {
                fontCache: 'global'
              }
            };
        </script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
<link href="/css/style.css" rel="stylesheet" />
</head>

<body>
    <div class="frontpage">
    <div class="blog" style="font-size:12pt;font-family:'Source Sans Pro'">
        <h2>
            <a style="title" href="../papersums.html"><i class="fa fa-arrow-left"></i></a>
        </h2>
        <h3>Thermodynamic Cost and Benefit of Memory</h3>
        <h4>Paper by Susanne Still, Summary and Illustrations by Farita Tasnim</h4>
        
        <p>&nbsp;</p>

        <h5><i class="fas fa-heart"></i>&nbsp; Feels</h5>
            <p>I thoroughly enjoyed reading this &nbsp;<span style="color:salmon"><i class="fas fa-grin-stars"></i>&nbsp;<b> stunning</b></span> work by Susanne Still. The generality of the theoretical framework developed herein, as well as its uniqueness due to accounting for &nbsp;<span style="color:salmon"><i class="fas fa-low-vision"></i>&nbsp;<b> partial observability</b></span>, helped set the spark for many ideas in my mind. &nbsp;<i class="fas fa-exclamation-circle"></i>&nbsp; I recently watched a talk (it was so good!) by Jinghui Liu, a graduate student in the lab I'm coadvised in (FakhriLab@MIT), and her talk about topological dynamics and bosonic phases on the surface of a cell membrane got me to thinking about treating topological defects (in wave propagation on the cell membrane) as effective particles in a nonequilibrium statistical mechanics setup. Re: this paper, can we think about the information processing capacity of a cell from the abstract topological sense?</p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-apple-alt"></i>&nbsp; In a Nutshell</h5>
            <p>A theoretical framework for the thermodynamics of memories interacting with partially observable systems demonstrates that &nbsp;<span style="color:salmon"><i class="fas fa-fire-extinguisher"></i>&nbsp;<b>minimizing the lower bound of the dissipation</b></span> of such an information engine leads to an &nbsp;<span style="color:salmon"><i class="fas fa-table"></i>&nbsp;<b> optimal data representation strategy</b></span> when available knowledge and system manipulability are limited.</p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-dragon"></i>&nbsp; Szilard Engine</h5>
        <p>A particle is trapped in a box with a partition in the middle. If an agent who is observing the system has knowledge of which half of the box the particle lies in, it can perform work extraction (of amount &nbsp;\(~kT\ln 2\)) by isothermal expansion. However, models such as this <span style="color:salmon"><b>assume</b></span> that <b>(1) all agent choices are optimal</b> and <b>(2) all relevant degrees of freedom are observable</b>. This heavily <span style="color:salmon"><b>limits</b></span> the discussion of information engines to cases where all captured information can be converted into useful work. Most of the time <span style="color:salmon"><b>in real systems, agents must act on partial knowledge</b></span> and predict quantities relevant to work extraction from the limited available data. Modeling this more realistic aspect of information engines would be useful for systems such as <span style="color:salmon"><b>biomolecular machines</b></span> and <span style="color:salmon"><b>engineered nanotechnology</b></span>. </p>

            <h4><img src="anims/szilard.gif" alt="Szilard Engine" width="200px" height="auto"></h4>
        
        <p>&nbsp;</p>

        <h5><i class="fas fa-fan"></i>&nbsp; General Information Engine Setup</h5>
        <p>The following <span style="color:#E69A22"><b>conventions</b></span> are used:</p>
        
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>The <b>cost of information acquisition and decision making are included in the energy accounting</b> (otherwise information can be viewed as fuel supplied from the outside).</li>
                <li>The engine is <b>allowed to make use of temperature differences</b>.</li>
                <li>Energy flows <i>into</i> a system are <b>positive</b>.</li>
            </ul>
            
            <p>The information engine contains the following <span style="color:#E69A22"><b>components</b></span>:</p>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>A <b>partially observable system</b>, with microstate denoted by a random variable \(~Z\) with realizations \(~z \in Z\)</li>
                <li>An <b>agent</b> implemented by another physical system which turnes measurements into a stable memory denoted by a random variable \(~M\) with realizations \(~m \in M\). The memory is used to decide on a work extraction protocol.</li>
                <li>A <b>work extraction device</b> that allows the agent to couple useful energy out of the system.</li>
            </ul>
        
            <p>Each <span style="color:indigo"><b>cycle</b></span> <img src="https://latex.codecogs.com/gif.latex?\inline&space;\dpi{100}&space;\fn_jvn&space;\small&space;i" title="\small i" /> runs as follows:</p>
        
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>From  \(~t_0^i \rightarrow t_1^i\): The <span style="color:#E69A22"><b>agent performs measurement and writes it into memory</b></span>. This protocol changes external control parameters on the memory as a <span style="color:#E69A22"><b>f(observable data)</b></span>. During this process the engine is connected to a heat bath at temperature \(~T\), and the average amount of work done on the memory is  \(~\langle W_M \rangle\), while  the average amount of heat dissipated is  \(~\langle -Q_M \rangle\). </li>
                <li>From  \(~t_1^i \rightarrow t_2^i\), the <span style="color:#E69A22"><b>temperature of the heat bath is adjusted</b></span> to temperature \(~T'\).</li>
                <li>From  \(~t_2^i \rightarrow t_3^i\), <span style="color:#E69A22"><b>work extraction</b></span> occurs via a protocol that is a <span style="color:#E69A22"><b>f(agent's memory state)</b></span>. During this process, the average amount of work extracted from the system is  \(~\langle -W_E \rangle\) and the average heat absorbed from the heat bath is \(~\langle Q_E \rangle\). After this process, the memory no longer has any exploitable correlations with the system.</li>
                <li>From \(~t_3^i \rightarrow t_0^{i+1}\), the <span style="color:#E69A22"><b>system is prepared for the next cycle</b></span>, by a <span style="color:#E69A22"><b>protocol which is invariant across cycles</b></span>. This process is assumed to not require any work. In the example of the Szilard engine, this was removing the partition and re-inserting it into the middle of the box.</li>
            </ul>
        
<!--        <p>&nbsp;</p>-->

        <h5><i class="fas fa-atom"></i>&nbsp; Free Energy Changes</h5>
            <p>The <span style="color:#cfb61e"><b>free energy change</b></span> at a time \(~t\) depends on the energy and entropy averaged over the joint distribution over the system states and memory states: \(~F_t = \langle E_t (m,z) \rangle_{p_t (m,z)} - kTH_t\), where the <span style="color:#cfb61e"><b>Shannon entropy</b></span> is defined as: \(~H_t = - \langle \ln(p_t (m,z)) \rangle_{p_t (m,z)}\)</p>
        
            <p>We further establish that: </p>
            $$\Delta F_M \equiv F_{t_1} - F_{t_0} = \Delta E_M (= W_M + Q_M) -kT\Delta H_M$$
            $$\Delta F_E \equiv F_{t_3} - F_{t_2} = \Delta E_E (= W_E + Q_E) -kT\Delta H_E$$
            
        <p>&nbsp;</p>

        <h5><i class="fas fa-chart-pie"></i>&nbsp; System Decomposition</h5>
            <p>The system can be decomposed into <span style="color:#cfb61e"><b>observable (write-able into memory)</b></span> and non-observable components: \(~Z = (X, \bar X)\). It can also be decomposed into <span style="color:#cfb61e"><b>manipulable (relevant for work extraction)</b></span> and non-manipulable components: \(~Z = (Y, \bar Y)\). The <span style="color:#cfb61e"><b>mutual information \(~ I(X,Y) \geq 0\)</b></span> in order to extract work.</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-wrench"></i>&nbsp; System Manipulation</h5>
            <p>The system cannot be manipulated in a way that changes anything but \(~y \in Y\), i.e. the state of the non-manipulable components \(~ \bar y \in \bar Y \) is unaffected by the state of the manipulable components and the state of the memory during the work extraction period.</p>
            $$p_{t_2}(\bar y \mid y,m) = p_{t_3}(\bar y \mid y,m) = p(\bar y \mid y,m)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-table"></i>&nbsp; Data Representation</h5>
            <p>The <span style="color:#cfb61e"><b>stochastic mapping (i.e. the data representation) \(~ p(m \mid x)\) from observable data to memory state</b></span> is independent of unobservable data:</p>
            $$ p_{t_1}(m \mid z) = p_{t_1}(m \mid x, \bar x) = p_{t_1}(m \mid x) \equiv p(m \mid x)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fab fa-hive"></i>&nbsp; Marginal Distributions</h5>
            <p>The <span style="color:#cfb61e"><b>marginal distributions for the system are invariant</b></span> to all changes performed on the system. Thus \(~ \forall k \in \{1,2,3,4\}\)</p>
            $$ p_{t_k}(z) = p(z), ~~~p_{t_k}(y) = \sum_{\bar y} p_{t_k}(y, \bar y) = p(y), ~~~p_{t_k}(x) = \sum_{\bar x} p_{t_k}(x, \bar x) = p(x)$$
        
            <p>The <span style="color:#cfb61e"><b>preparation step introduces a hidden variable \(~v\)</b></span> into the system, which if discovered, the system appears in a nonequilibrium state that can be exploited during work extraction.</p>
            $$p_{t_0}(y) = \sum_v p(y \mid v) p(v)$$
        
            <p>The <span style="color:#cfb61e"><b>marginal probability distribution of the memory </b></span> derives only from the statistical average over measurement outcomes.</p>
            $$p_{t_k}(m) = \sum_x p(m \mid x) p(x)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-chart-line"></i>&nbsp; Inference</h5>
        <p>The <span style="color:#cfb61e"><b>agent's ability to predict the quantities relevant to work extraction from the memory </b></span> derives from a statistical average over measurement outcomes: </p>
            $$p_{t_2}(y \mid m) = \sum_x p(y \mid x) p(m \mid x) p(x)$$
            <p>utilizing that if a measurement outcome is given, the memory adds no new relevant information: \(~p(y \mid m, x) = p(y \mid x)\)</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-money-bill-alt"></i>&nbsp; Thermodynamic Cost of Memory</h5>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>At \(~t_0\), the system and memory are uncorrelated: \(~p_{t_0}(m,z) = p_{t_0}(m)p_{t_0}(z) = p(m)p(z) \)</li>
                <li>At \(~t_1\), <span style="color:#a6b81b"><b>after the memory is constructed, the memory possesses useful correlations with the system: </b></span> \(~p_{t_1}(m,z) = p_{t_1}(m \mid x, \bar x)p_{t_1}(z) = p(m \mid x)p(z) \)</li>
                <li>Since <span style="color:#a6b81b"><b>the uncertainty about the system decreases, the entropy decreases</b></span> by the amount of mutual information captured in the memory about the observable data: 
                $$\Delta H_M = H[M,X] - H[M] = -I[M,X] $$</li>
                <li>This process happens at a temperature \(~T\), so <b>the free energy change associated with memory construction</b> is \(~\Delta F_M = W_M + Q_M + kTI[M,X]\)</li>
                <li>By the Second Law, the work required to construct the memory must be greater than or equal to the resulting free energy change of the memory: \(~ W_M \geq \Delta F_M\). Thus, <span style="color:#a6b81b"><b>operating a memory requires, at minimum, a dissipation proportional to the amount of information retained</b></span>: \(~ -Q_M \geq kTI[M,X]\) </li>
            </ul>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-gift"></i>&nbsp; Thermodynamic Gain from Memory</h5>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>At \(~t_2\), the beginning of work extraction, <span style="color:#a6b81b"><b>the correlations between the system and memory may provide useful information for predicting quantities relevant to work extraction</b></span>: \(~ p_{t_2}(m,z) = p_{t_2} (\bar y \mid y, m) p_{t_2}(y \mid m) p_{t_2}(m) = p(\bar y \mid y, m) p(y \mid m) p(m) \) </li>
                <li>At \(~t_3\), the end of the work extraction protocol, <span style="color:#a6b81b"><b>all such correlations between manipulable system components and the memory are then gone</b></span> and thus: \(~ p_{t_3}(m,z) = p_{t_3} (\bar y \mid y, m) p_{t_3}(y) p_{t_3}(m) = p(\bar y \mid y, m) p(y) p(m) \) </li>
                <li>This again <span style="color:#a6b81b"><b>increases uncertainy about the system, and thus the entropy of the joint system increases</b></span> by an amount equal to the mutual information between the memory and manipulable system components 
                $$\Delta H_E = H[Y] - H[Y \mid M] = I[M,Y]$$ </li>
                <li>This process occurs at temperature \(~T'\), so <b>the free energy change associated with work extraction</b> is \(~\Delta F_E = W_E + Q_E - kT'I[M,Y] \)</li>
                <li>By the second law, one can only extract an amount of work less than or equal to the decrease in free energy of the system during the extraction process: \(~W_E \geq \Delta F_E\) (remember that both quantities are negative). Thus, the <span style="color:#a6b81b"><b>amount of heat absorbed that can be absorbed by the system and turned into work is bounded proportionally to the amount of information retained in the agent's memory that's relevant to the system components pertinent to work extraction </b></span>: \(~Q_E \leq kT'I[M,Y] \). This holds if the relevant quantities are fully observable.</li>
                <li>However, generally, the system components that are observable by the agent do not provide full information regarding the components relevant to work extraction. As such, all of the energertic cost of running the memory may not be recovered. <b>Out of the information \(~I_{mem} = I[M,X]\) captured in the memory, only some bits \(~I_{rel} = I[M,Y] \) are useful for prediction. The rest, \(~I_{irrel} = I_{mem} - I_{rel} \geq 0 \), is irrelevant information.</b></li>
                <li>So, we can actually <span style="color:#a6b81b"><b>relax the upper bound on heat absorption: \(~Q_E \leq kT'I[M,X]\).</b></span></li>
                <li>&nbsp;<i class="fas fa-question-circle"></i>&nbsp; Does this imply that more heat can be absorbed and converted into work when we don't have full information? It cannot be the case, because full work extraction can only occur in the ideal case when we have full information about relevant quantities in the system, or at least when \(~Y \subset X\). So how to think about this relaxed upper bound?</li>
                <li>At this point, it may be helpful to recall that the mutual information of two random variables is
                $$I[X,Y] = D_{KL}(P_{(X,Y)} || P_X \otimes P_Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \ln(\frac{p(x,y)}{p(x)p(y)}) $$
                i.e. that the mutual information signifies the loss in correlations between two random variables. &nbsp;<i class="fas fa-exclamation-circle"></i>&nbsp; A fun related but unrelated theory is that the KL divergence between probability distributions of the forward and reverse trajectories of a process define the arrow of time [cite Crooks].</li>
            </ul>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-fire"></i>&nbsp; Lower Bound on Dissipation</h5>
        <p>If the information engine is connected to a heat bath of constant temperature for an entire cycle, it dissipates an average amount of heat \(~ -Q = -Q_M - Q_E \geq kTI_{irrel} \). <span style="color:lightseagreen"><b>The irrelevant information retained in the memory sets a lower limit on the dissipation.</b></span> This lower bound is zero only when no irrelevant information is retained.</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-box"></i>&nbsp; An Illustrative Example</h5>
        <p>Let's talk about an example now to make calculations more concrete. Consider the Szilard engines below. In the left box, knowledge of the \(~x\) coordinate of the particle provides no information about the \(~y\) coordinate (i.e. which half of the box the particle is located in). However, in the right box, <span style="color:lightseagreen"><b>introducing geometric constraints within the box creates correlations between the degrees of freedom</b></span>. As such, an agent observing the \(~x\) coordinate would have a non-trivial probability distribution for the \(~y\) coordinate, and thus would be able to engage in predictive inference to perform useful work.</p>
        
            <p>&nbsp;</p>
            <h4><img src="anims/boxcorrelations.gif" alt="Szilard Engine" width="300px" height="auto"></h4>
        
        <p>Now note that <span style="color:lightseagreen"><b>the coarse-graining strategy chosen for data representation in the agent's memory will affect how much work can be extracted</b></span> from the setup. For example, representing the particle's \(~x\) coordinate with a 2-state (left) or 3-state (right) memory affect's the agent's capacity for predictive inference. The wall will be moved to the side believed to be empty with higher probability. This will be <span style="color:lightseagreen"><b>incorrect (and result in compression instead of expansion) with probability \(~q(m)\). Accordingly, leaving a fractional volume \(~\rho (m) V\) unused is a good strategy.</b></span> </p>
        
            <p>&nbsp;</p>
            <h4><img src="anims/box2or3mem.gif" alt="Szilard Engine" width="300px" height="auto"></h4>
        
            <p>Thus, given \(~m\), we extract on average an amount of work:
            $$kT'(1-q(m))\ln (\frac{V - \rho (m)V}{\frac{V}{2}}) + kT'q(m)\ln (\frac{\rho (m)V}{\frac{V}{2}})$$
        
            This is maximized when \(~\rho(m) = q(m)\). This leads to a total average extracted work, which saturates the bound derived earlier from the second law, because an isothermal transformation leaves the average energy of an ideal gas unchanged, so \(~-W_E = Q_E\):
            $$-W_E = kT' \sum_m p(m) [\ln 2 + (1-q(m))\ln (1-q(m)) + q(m)\ln (q(m))]$$
            $$=kT'(H[Y] - H[Y \mid M])$$
            $$=kT'I[M,Y]$$</p>
        
            <ul>
                <li><span style="color:lightseagreen"><b>For a 3-state memory</b></span>, \(~m \in \{1, 2, 3\}\), and 
                $$~q(m) = \left\{\begin{array}{ll} 0 & \quad m = 1, 3 \\ \frac{1}{2} & \quad m = 2 \end{array} \right.$$ 
                    Thus, \(~I[X,Y] = \frac{2}{3} \ln 2\) and at most \(~kT \frac{2}{3} \ln 2\) amount of work can be extracted at a cost of \(~kT \ln 3\) to run the memory. Therefore <span style="color:lightseagreen"><b>the lower limit on dissipation is \(~ -Q \geq kT'I_{irrel} = kT(\ln 3 - \frac{2}{3} \ln 2) \approx 0.64kT\).</b></span></li>
        
                <li><span style="color:lightseagreen"><b>For a 2-state memory</b></span>, \(~m \in \{1, 2\}\), and 
                $$~q(m) = \frac{1}{6}, m = 1,2$$ 
                    Thus, \(~I[X,Y] = \frac{5}{6} \ln 5 - \ln 3\) and at most \(~kT [\frac{5}{6} \ln 5 - \ln 3]\) amount of work can be extracted at a cost of \(~kT \ln 2\) to run the memory. Therefore <span style="color:lightseagreen"><b>the lower limit on dissipation is \(~ -Q \geq kT'I_{irrel} = kT(\ln 6 - \frac{5}{6} \ln 5) \approx 0.45kT\).</b></span></li>
            </ul>
        
        <p>We therefore see that the less-detailed 2-state memory is more thermodynamically efficient because the cost of running a 3-state memory is relatively high.</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-temperature-high"></i>&nbsp; Access to Two Temperatures</h5>
        <p>However, <span style="color:#678BBA"><b>given access to two temperatures, the 3-state memory can become more efficient</b></span>. Let's say we form the memory at a temperature \(~T\) and extract work at a temperature \(~T' > T\). A 3-state memory becomes advantageous when:
            $$\alpha \equiv \frac{T'}{T} > \alpha^* = -\frac{cost(2SM) - cost(3SM)}{gain(2SM)-gain(3SM)} = \frac{\ln 3 - \ln 2}{\ln 3 + \frac{2}{3} \ln 2 - \frac{5}{6} \ln 5} \approx 1.847$$
            While an isothermal information engine can at best only recover the energy required to run the memory, <span style="color:#678BBA"><b>an information engine operated at two temperatures can produce non-negative work output</b></span>. The earlier lower bound on dissipation generalizes to \(~-Q \geq k(TI_{mem} - T'I_{rel})\). There are some key assumptions in the use of two temperatures:</p>
        
            <ul>
                <li>The <b>heating and cooling of the information engine do not destroy the correlations</b> between the memory and the system.</li>
                <li><b>No additional degrees of freedom are unlocked</b> at higher temperatures.</li>
                <li>The heating and cooling steps together <b>do not result in a net influx of heat.</b></li>
            </ul>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-code"></i>&nbsp; Protocols</h5>
        <p>To find <span style="color:#2A44E0"><b>the best strategy for data representation in memory</b></span>, the agent must:
            $$\min_{p(m,x)} [I[M,X] - \alpha I[M,Y]]~~ s.t. ~\sum_m p(m\mid x) = 1$$
            <span style="color:#2A44E0"><b>\(~ \alpha \) </b></span> determines the relative importance of detail (finer coarse-graining) in the data representation, and <span style="color:#2A44E0"><b>can also be interpreted as a Lagrange multiplier that controls the tradeoff between conciseness and relevance</b></span>. The generalized Szilard information engine presented above can be run in a process akin to a Carnot process (wow I'm surprised that part of TD is useful XD - it is the most boring part for me). </p>
        
            <ul>
                <li>The <span style="color:#2A44E0"><b>data representation step can be an isothermal transformation</b></span> at temperature \(~T\), implementable with \(~ \Delta E_M = 0\), implying \(~W_M = - Q_M = kTI_{mem}\).</li>
                <li>The <span style="color:#2A44E0"><b>work extraction protocol</b></span> can be implemented as follows in order to respect the assumptions stated earlier:
                    <ul>
                        <li>Isolate the box from the heat bath and perform and <span style="color:#2A44E0"><b>isentropic (adiabatic and reversible) compression of the entire box</b></span>, taking it from volume \(~V \rightarrow V'\), and raising the temperature to \(~T'\). For such a process, \(~ \frac{V}{V'} = (\frac{T'}{T})^{\frac{d}{2}} \), where \(~d\) is the number of degrees of freedom of the box.</li>
                        <li>Connect the engine to a heat bath at temperature \(~T'\) and <span style="color:#2A44E0"><b>extract work isothermally</b></span> by moving the partition to the most probable empty direction, leaving the optimized fractional volume \(~ \rho (m) V\).</li>
                        <li>Isolate the box and perform isentropic expansion from \(~V' \rightarrow V\), which lowers the temperature to \(~T\).</li>
                        <li>Remove the partition and re-insert it in the middle.</li>
                    </ul>
                Overall, this protocol will turn the maximal possible amount of heat into work \(~Q_E = -W_E = kT'I_{rel}\)</li>
            </ul>
            
            <p>Thus the net work produced from the above process is \(~ -W_E - W_M = k(T'I_{rel} - TI_{mem})\), which saturates our lower bound on dissipation. The engine has an efficiency:
            $$\eta = 1 - \frac{T}{T'} \frac{I_{mem}}{I_{rel}} = \eta_c - \frac{T}{T'} \frac{I_{irrel}}{I_{rel}}$$
                where \(~ \eta_c = 1 - \frac{T}{T'}\). <span style="color:#2A44E0"><b>The efficiency of the engine is only non-negative when \(~ \frac{I_{rel}}{I_{mem}} \geq \frac{T}{T'}\).</b></span> So the strategies to increase efficiency are to increase \(~ I_{rel}\) or increase \(~ T'\), the former of which is easier since it simply requires picking the smartest data representation strategy given the available data.</p>
        
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-lightbulb"></i>&nbsp; Closing Thoughts</h5>
        <p>This method can be applied to many different systems, one of which is the <span style="color:#2AE0C6"><b>Boltzmann machine</b></span>, a machine learning algorithm in which input patterns drive the neural network from a parameter-dependent equilibrium state \(~q_{\theta}\) to a nonequilibrium state \(~ p\). The change in free energy during this process \(~ \Delta F = kTD_{KL}[p || q_{\theta}]\) is dissipated during the relaxation process involved in predicting labels on new patterns. Finding the optimal parameters that minimize \(~ D_{KL}[p || q_{\theta}]\) minimizes the lower bound on average dissipation encountered during prediction.</p>
        
        <p>This paper focuses on optimized an information engine for energy efficiency, but <span style="color:#2AE0C6"><b>a more realistic information engine would undergo multivariate optimization, accounting for other variables such as speed, accuracy, robustness</b></span>. There are tradeoffs to all of these factors. This work can also be applied to quantum systems, and I will have to read the references cited in the paper to get a sense of how :) </p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-book-open"></i>&nbsp; Original Paper</h5>
        <p>&nbsp;</p>
            <iframe src="Still_Memory.pdf" width="100%" height="600px"></iframe>

        <h2>
            <a style="title" href="../../../../index.html"><i class="fa fa-home"></i></a>
        </h2>
     </div>
     </div>
</body>


</html>
