<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Farita | PoLS</title>
<link href='http://fonts.googleapis.com/css?family=Droid+Serif' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Droid+Sans' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300'rel='stylesheet' type='text/css'>
		<link href="/css/style.css" rel="stylesheet" />
        <script src="https://kit.fontawesome.com/3d48ed8956.js" crossorigin="anonymous"></script>
        <script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(~', '\\)']]
              },
              svg: {
                fontCache: 'global'
              }
            };
        </script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
<link href="/css/style.css" rel="stylesheet" />
</head>

<body>
    <div class="frontpage">
    <div class="blog" style="font-size:12pt;font-family:'Source Sans Pro'">
        <h2>
            <a style="title" href="../papersums.html"><i class="fa fa-arrow-left"></i></a>
        </h2>
        <h3>Thermodynamic Cost and Benefit of Memory</h3>
        <h4>Paper by Susanne Still, Summary and Illustrations by Farita Tasnim</h4>
        
        <p>&nbsp;</p>

        <h5><i class="fas fa-heart"></i>&nbsp; Feels</h5>
            <p>I thoroughly enjoyed reading this &nbsp;<span style="color:#630C90"><i class="fas fa-grin-stars"></i>&nbsp;<b> stunning</b></span> work by Susanne Still. The generality of the theoretical framework developed herein, as well as its uniqueness due to accounting for &nbsp;<span style="color:#900C7B"><i class="fas fa-low-vision"></i>&nbsp;<b> partial observability</b></span>, helped set the spark for many ideas in my mind, which I'll sprinkle throughout my summary below.</p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-apple-alt"></i>&nbsp; In a Nutshell</h5>
            <p>A theoretical framework for the thermodynamics of memories interacting with partially observable systems demonstrates that &nbsp;<span style="color:#900C39"><i class="fas fa-fire-extinguisher"></i>&nbsp;<b>minimizing the lower bound of the dissipation</b></span> of such an information engine leads to an &nbsp;<span style="color:#90210C"><i class="fas fa-table"></i>&nbsp;<b> optimal data representation strategy</b></span> when available knowledge and system manipulability are limited.</p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-dragon"></i>&nbsp; Szilard Engine</h5>
        <p>A particle is trapped in a box with a partition in the middle. If an agent who is observing the system has knowledge of which half of the box the particle lies in, it can perform work extraction (of amount &nbsp;\(~kT\ln 2\)) by isothermal expansion. However, models such as this <span style="color:#90630C"><b>assume</b></span> that <b>(1) all agent choices are optimal</b> and <b>(2) all relevant degrees of freedom are observable</b>. This heavily <span style="color:#90630C"><b>limits</b></span> the discussion of information engines to cases where all captured information can be converted into useful work. Most of the time <span style="color:#7B900C"><b>in real systems, agents must act on partial knowledge</b></span> and predict quantities relevant to work extraction from the limited available data. Modeling this more realistic aspect of information engines would be useful for systems such as <span style="color:#0C904F"><b>biomolecular machines</b></span> and <span style="color:#0C904F"><b>engineered nanotechnology</b></span>. </p>

            <h4><img src="anims/szilard.gif" alt="Szilard Engine" width="200px" height="auto"></h4>
        
        <p>&nbsp;</p>

        <h5><i class="fas fa-fan"></i>&nbsp; General Information Engine Setup</h5>
        <p>The following <span style="color:blue"><b>conventions</b></span> are used:</p>
        
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>The <b>cost of information acquisition and decision making are included in the energy accounting</b> (otherwise information can be viewed as fuel supplied from the outside).</li>
                <li>The engine is <b>allowed to make use of temperature differences</b>.</li>
                <li>Energy flows <i>into</i> a system are <b>positive</b>.</li>
            </ul>
            
            <p>The information engine contains the following <span style="color:red"><b>components</b></span>:</p>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>A <b>partially observable system</b>, with microstate denoted by a random variable \(~Z\) with realizations \(~z \in Z\)</li>
                <li>An <b>agent</b> implemented by another physical system which turnes measurements into a stable memory denoted by a random variable \(~M\) with realizations \(~m \in M\). The memory is used to decide on a work extraction protocol.</li>
                <li>A <b>work extraction device</b> that allows the agent to couple useful energy out of the system.</li>
            </ul>
        
            <p>Each <span style="color:indigo"><b>cycle</b></span> <img src="https://latex.codecogs.com/gif.latex?\inline&space;\dpi{100}&space;\fn_jvn&space;\small&space;i" title="\small i" /> runs as follows:</p>
        
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>From  \(~t_0^i \rightarrow t_1^i\): The <span style="color:#FFC300"><b>agent performs measurement and writes it into memory</b></span>. This protocol changes external control parameters on the memory as a <span style="color:#FFC300"><b>f(observable data)</b></span>. During this process the engine is connected to a heat bath at temperature \(~T\), and the average amount of work done on the memory is  \(~\langle W_M \rangle\), while  the average amount of heat dissipated is  \(~\langle -Q_M \rangle\). </li>
                <li>From  \(~t_1^i \rightarrow t_2^i\), the <span style="color:#FF5733"><b>temperature of the heat bath is adjusted</b></span> to temperature \(~T'\).</li>
                <li>From  \(~t_2^i \rightarrow t_3^i\), <span style="color:#C70039"><b>work extraction</b></span> occurs via a protocol that is a <span style="color:#C70039"><b>f(agent's memory state)</b></span>. During this process, the average amount of work extracted from the system is  \(~\langle -W_E \rangle\) and the average heat absorbed from the heat bath is \(~\langle Q_E \rangle\). After this process, the memory no longer has any exploitable correlations with the system.</li>
                <li>From \(~t_3^i \rightarrow t_0^{i+1}\), the <span style="color:#900C3F"><b>system is prepared for the next cycle</b></span>, by a <span style="color:#900C3F"><b>protocol which is invariant across cycles</b></span>. This process is assumed to not require any work. In the example of the Szilard engine, this was removing the partition and re-inserting it into the middle of the box.</li>
            </ul>
        
<!--        <p>&nbsp;</p>-->

        <h5><i class="fas fa-atom"></i>&nbsp; Free Energy Changes</h5>
            <p>The <span style="color:mediumvioletred"><b>free energy change</b></span> at a time <img src="https://latex.codecogs.com/gif.latex?\inline&space;\dpi{100}&space;\fn_jvn&space;\small&space;t" title="\small t" /> depends on the energy and entropy averaged over the joint distribution over the system states and memory states: \(~F_t = \langle E_t (m,z) \rangle_{p_t (m,z)} - kTH_t\), where the <span style="color:hotpink"><b>Shannon entropy</b></span> is defined as: \(~H_t = - \langle \ln(p_t (m,z)) \rangle_{p_t (m,z)}\)</p>
        
            <p>We further establish that: </p>
            $$\Delta F_M \equiv F_{t_1} - F_{t_0} = \Delta E_M (= W_M + Q_M) -kT\Delta H_M$$
            $$\Delta F_E \equiv F_{t_3} - F_{t_2} = \Delta E_E (= W_E + Q_E) -kT\Delta H_E$$
            
        <p>&nbsp;</p>

        <h5><i class="fas fa-chart-pie"></i>&nbsp; System Decomposition</h5>
            <p>The system can be decomposed into <span style="color:yellowgreen"><b>observable (write-able into memory)</b></span> and non-observable components: \(~Z = (X, \bar X)\). It can also be decomposed into <span style="color:lightseagreen"><b>manipulable (relevant for work extraction)</b></span> and non-manipulable components: \(~Z = (X, \bar X)\). The <span style="color:#0C8290"><b>mutual information \(~ I(X,Y) \geq 0\)</b></span> in order to extract work.</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-wrench"></i>&nbsp; System Manipulation</h5>
            <p>The system cannot be manipulated in a way that changes anything but \(~y \in Y\), i.e. the state of the non-manipulable components \(~ \bar y \in \bar Y \) is unaffected by the state of the manipulable components and the state of the memory during the work extraction period.</p>
            $$p_{t_2}(\bar y \mid y,m) = p_{t_3}(\bar y \mid y,m) = p(\bar y \mid y,m)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-table"></i>&nbsp; Data Representation</h5>
            <p>The <span style="color:salmon"><b>stochastic mapping (i.e. the data representation) \(~ p(m \mid x)\) from observable data to memory state</b></span> is independent of unobservable data:</p>
            $$ p_{t_1}(m \mid z) = p_{t_1}(m \mid x, \bar x) = p_{t_1}(m \mid x) \equiv p(m \mid x)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fab fa-hive"></i>&nbsp; Marginal Distributions</h5>
            <p>The <span style="color:#E69A22"><b>marginal distributions for the system are invariant</b></span> to all changes performed on the system. Thus \(~ \forall k \in \{1,2,3,4\}\)</p>
            $$ p_{t_k}(z) = p(z), ~~~p_{t_k}(y) = \sum_{\bar y} p_{t_k}(y, \bar y) = p(y), ~~~p_{t_k}(x) = \sum_{\bar x} p_{t_k}(x, \bar x) = p(x)$$
        
            <p>The <span style="color:#E6CB22"><b>preparation step introduces a hidden variable \(~v\)</b></span> into the system, which if discovered, the system appears in a nonequilibrium state that can be exploited during work extraction.</p>
            $$p_{t_0}(y) = \sum_v p(y \mid v) p(v)$$
        
            <p>The <span style="color:#D0E622"><b>marginal probability distribution of the memory </b></span> derives only from the statistical average over measurement outcomes.</p>
            $$p_{t_k}(m) = \sum_x p(m \mid x) p(x)$$
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-chart-line"></i>&nbsp; Inference</h5>
        <p>The <span style="color:lightseagreen"><b>agent's ability to predict the quantities relevant to work extraction from the memory </b></span> derives from a statistical average over measurement outcomes: </p>
            $$p_{t_2}(y \mid m) = \sum_x p(y \mid x) p(m \mid x) p(x)$$
            <p>utilizing that if a measurement outcome is given, the memory adds no new relevant information: \(~p(y \mid m, x) = p(y \mid x)\)</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-money-bill-alt"></i>&nbsp; Thermodynamic Cost of Memory</h5>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>At \(~t_0\), the system and memory are uncorrelated: \(~p_{t_0}(m,z) = p_{t_0}(m)p_{t_0}(z) = p(m)p(z) \)</li>
                <li>At \(~t_1\), <span style="color:#678BBA"><b>after the memory is constructed, the memory possesses useful correlations with the system: </b></span> \(~p_{t_1}(m,z) = p_{t_1}(m \mid x, \bar x)p_{t_1}(z) = p(m \mid x)p(z) \)</li>
                <li>Since <span style="color:#4974AC"><b>the uncertainty about the system decreases, the entropy decreases</b></span> by the amount of mutual information captured in the memory about the observable data: 
                $$\Delta H_M = H[M,X] - H[M] = -I[M,X] $$</li>
                <li>This process happens at a temperature \(~T\), so <span style="color:#2A5C9E"><b>the free energy change associated with memory construction</b></span> is \(~\Delta F_M = W_M + Q_M + kTI[M,X]\)</li>
                <li>By the Second Law, the work required to construct the memory must be greater than or equal to the resulting free energy change of the memory: \(~ W_M \geq \Delta F_M\). Thus, <span style="color:#0C4590"><b>operating a memory requires, at minimum, a dissipation proportional to the amount of information retained</b></span>: \(~ -Q_M \geq kTI[M,X]\) </li>
            </ul>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-gift"></i>&nbsp; Thermodynamic Gain from Memory</h5>
            <ul style="font-size:12pt;font-family:'Source Sans Pro'">
                <li>At \(~t_2\), the beginning of work extraction, <span style="color:#BA67A3"><b>the correlations between the system and memory may provide useful information for predicting quantities relevant to work extraction</b></span>: \(~ p_{t_2}(m,z) = p_{t_2} (\bar y \mid y, m) p_{t_2}(y \mid m) p_{t_2}(m) = p(\bar y \mid y, m) p(y \mid m) p(m) \) </li>
                <li>At \(~t_3\), the end of the work extraction protocol, <span style="color:#AC4991"><b>all such correlations between manipulable system components and the memory are then gone</b></span> and thus: \(~ p_{t_3}(m,z) = p_{t_3} (\bar y \mid y, m) p_{t_3}(y) p_{t_3}(m) = p(\bar y \mid y, m) p(y) p(m) \) </li>
                <li>This again <span style="color:#900C6C"><b>increases uncertainy about the system, and thus the entropy of the joint system increases</b></span> by an amount equal to the mutual information between the memory and manipulable system components 
                $$\Delta H_E = H[Y] - H[Y \mid M] = I[M,Y]$$ </li>
                <li>This process occurs at temperature \(~T'\), so <span style="color:#7E0B5F"><b>the free energy change associated with work extraction</b></span> is \(~\Delta F_E = W_E + Q_E - kT'I[M,Y] \)</li>
                <li>By the second law, one can only extract an amount of work less than or equal to the decrease in free energy of the system during the extraction process: \(~W_E \geq \Delta F_E\) (remember that both quantities are negative). Thus, the <span style="color:#6C0951"><b>amount of heat absorbed that can be absorbed by the system and turned into work is bounded proportionally to the amount of information retained in the agent's memory that's relevant to the system components pertinent to work extraction </b></span>: \(~Q_E \leq kT'I[M,Y] \). This holds if the relevant quantities are fully observable.</li>
                <li>However, generally, the system components that are observable by the agent do not provide full information regarding the components relevant to work extraction. As such, all of the energertic cost of running the memory may not be recovered. <span style="color:#E02AD0"><b>Out of the information \(~I_{mem} = I[M,X]\) captured in the memory, only some bits \(~I_{rel} = I[M,Y] \) are useful for prediction. The rest, \(~I_{irrel} = I_{mem} - I_{rel} \geq 0 \), is irrelevant information.</b></span></li>
                <li>So, we can actually <span style="color:#E02A75"><b>relax the upper bound on heat absorption: \(~Q_E \leq kT'I[M,X]\).</b></span></li>
                <li>&nbsp;<i class="fas fa-question-circle"></i>&nbsp; Does this imply that more heat can be absorbed and converted into work when we don't have full information? It cannot be the case, because full work extraction can only occur in the ideal case when we have full information about relevant quantities in the system, or at least when \(~Y \subset X\). So how to think about this relaxed upper bound?</li>
                <li>At this point, it may be helpful to recall that the mutual information of two random variables is
                $$I[X,Y] = D_{KL}(P_{(X,Y)} || P_X \otimes P_Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \ln(\frac{p(x,y)}{p(x)p(y)}) $$
                i.e. that the mutual information signifies the loss in correlations between two random variables. &nbsp;<i class="fas fa-exclamation-circle"></i>&nbsp; A fun related but unrelated theory is that the KL divergence between probability distributions of the forward and reverse trajectories of a process define the arrow of time [cite Crooks].</li>
            </ul>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-fire"></i>&nbsp; Lower Bound on Dissipation</h5>
        <p>If the information engine is connected to a heat bath of constant temperature for an entire cycle, it dissipates an average amount of heat \(~ -Q = -Q_M - Q_E \geq kTI_{irrel} \). <span style="color:#E03A2A"><b>The irrelevant information retained in the memory sets a lower limit on the dissipation.</b></span> This lower bound is zero only when no irrelevant information is retained.</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-box"></i>&nbsp; An Illustrative Example</h5>
            <p>Let's talk about an example now to make calculations more concrete. Consider the Szilard engines below. In the left box, knowledge of the \(~x\) coordinate of the particle provides no information about the \(~y\) coordinate (i.e. which half of the box the particle is located in). However, in the right box, introducing geometric constraints within the box creates correlations between the degrees of freedom. As such, an agent observing the \(~x\) coordinate would have a non-trivial probability distribution for the \(~y\) coordinate, and thus would be able to engage in predictive inference.</p>
        
            <p>&nbsp;</p>
            <h4><img src="anims/boxcorrelations.gif" alt="Szilard Engine" width="300px" height="auto"></h4>
        
            <p>Now note that the coarse-graining strategy chosen for data representation in the agent's memory will affect how much work can be extracted from the setup. For example, representing the particle's \(~x\) coordinate with a 2-state (left) or 3-state (right) memory affect's the agent's capacity for predictive inference.</p>
        
            <p>&nbsp;</p>
            <h4><img src="anims/box2or3mem.gif" alt="Szilard Engine" width="300px" height="auto"></h4>
        
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-temperature-high"></i>&nbsp; Access to Two Temperatures</h5>
            <p>&nbsp;</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-code"></i>&nbsp; Protocols</h5>
            <p>&nbsp;</p>
        
        <p>&nbsp;</p>
        
        <h5><i class="fas fa-lightbulb"></i>&nbsp; Closing Thoughts</h5>
            <p>&nbsp;</p>

        <p>&nbsp;</p>

        <h5><i class="fas fa-book-open"></i>&nbsp; Original Paper</h5>
        <p>&nbsp;</p>
            <iframe src="Still_Memory.pdf" width="100%" height="600px"></iframe>

        <h2>
            <a style="title" href="../../../../index.html"><i class="fa fa-home"></i></a>
        </h2>
     </div>
     </div>
</body>


</html>
